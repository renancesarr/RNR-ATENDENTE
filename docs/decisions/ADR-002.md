# ADR-002: Estratégia de Retrieval-Augmented Generation (RAG)

- **Status**: Accepted
- **Context Timestamp**: 2025-10-12

## Contexto
O chatbot WhatsApp precisa enriquecer respostas geradas pela OpenAI com informações atualizadas sobre catálogo, políticas comerciais e histórico recente de conversas (últimos 90 dias). Já adotamos Postgres com `pgvector` como vetor store (ADR-20251012-vector-store-strategy) e RabbitMQ/Redis para eventos e cache. Precisamos definir:
- Fontes de conhecimento e periodicidade de ingestão.
- Formato de chunking/embeddings.
- Estratégia de busca (top-k, filtros) e fallback quando não houver contexto relevante.
- Limites de custo (tokens e chamadas de embedding/completion).

## Fatores de decisão
- Manter TTFR baixo (<= 3 s) com respostas contextualizadas.
- Controlar custo de chamadas OpenAI (embeddings + completions).
- Simplificar operação para MVP, reaproveitando infraestrutura existente.
- Garantir rastreabilidade das fontes de conhecimento usadas em cada resposta.

## Opções consideradas
1. **Pipeline RAG síncrono com Postgres/pgvector + embeddings OpenAI `text-embedding-3-small`**  
   - Prós: integra-se ao stack atual; latência previsível; custo baixo por embedding; fácil auditoria dos chunks retornados.  
   - Contras: depende da disponibilidade da API OpenAI; requer orquestração de ingestão incremental.  
   - Impacto: custo ~USD 0.02 por 1K chunks/mês; latência ~50–150 ms para busca; manutenção moderada (jobs de ingestão + limpeza).
2. **Pipeline híbrido (Redis cache quente de chunks + Postgres frio)**  
   - Prós: consultas ainda mais rápidas (sub-20 ms) e possibilidade de TTL por canal.  
   - Contras: duplica esforço de sincronização; risco de inconsistência; aumenta consumo de memória Redis.  
   - Impacto: custo em RAM; manutenção alta (jobs de sync, invalidation).
3. **RAG gerenciado (ex.: Azure Cognitive Search, Pinecone com pipelines prontos)**  
   - Prós: ingestão simplificada, escalabilidade automática, métricas prontas.  
   - Contras: custo recorrente elevado; dependência de cloud específica; integrações extras com MVP self-hosted.  
   - Impacto: custo médio/alto; latência baixa; manutenção baixa porém com contratos/SLA.

## Decisão
Adotamos a opção 1: pipeline síncrono com Postgres/pgvector e embeddings `text-embedding-3-small`. Chunking em 500 tokens com overlap de 50 para catálogo e documentos longos; conversas são agregadas por janela de 10 mensagens relevantes. O serviço de orquestração irá:
1. Receber eventos de atualização (catalogo, políticas, novas conversas) via RabbitMQ.
2. Normalizar e deduplicar conteúdo antes de gerar embeddings.
3. Persistir no Postgres tabelas `knowledge_chunks` (conteúdo, metadados, timestamp, hash) e `knowledge_sources`.
Para cada atendimento, a busca vetorial retornará `top_k = 5` chunks filtrados por idioma/tenant. Caso a similaridade máxima < 0.75, aplicaremos fallback para resposta padrão (roteiro base) e registraremos evento para curadoria humana.

## Consequências
- Positivas: baixo custo operacional; integração direta com ferramentas existentes; rastreabilidade via tabelas Postgres.  
- Positivas: fácil ajuste de parâmetros (`top_k`, limiar de similaridade) sem reindexação massiva.  
- Riscos: atraso na ingestão se pipeline falhar; dependência da disponibilidade da OpenAI para embeddings.  
- Mitigações: implementar DLQ no RabbitMQ para eventos de ingestão; cachear embeddings recentes; monitorar custos mensais (limite inicial USD 150/mês).

## Follow-up
- [ ] Implementar serviço de ingestão com idempotência (hash do conteúdo) e DLQ para erros.  
- [ ] Criar dashboards de custo (OpenAI) e latência da busca vetorial.  
- [ ] Automatizar auditoria semanal de chunks órfãos ou desatualizados.  
- [ ] Documentar playbook de fallback manual quando similaridade < 0.75 gerar gaps de resposta.
